# --- MSRA specific parameters ---

# use epsilon greedy action selector
action_selector: "epsilon_greedy"
epsilon_start: 1.0
epsilon_finish: 0.05
epsilon_anneal_time: 50000
evaluation_epsilon: 0.0

runner: "episode"
# runner: "parallel"
# batch_size_run: 8

buffer_size: 5000

# update the target network every {} episodes
target_update_interval_or_tau: 200


obs_agent_id: True
obs_last_action: False
obs_individual_obs: False

# use the Q_Learner to train
standardise_returns: False
standardise_rewards: True

# Focus on: mac, agent, learner, mixer, use_rnn
mac: "DRMAC_mac"
agent: "DRMAC"
agent_output_type: "q"
learner: "DRMAC_learner"
double_q: True
mixer: "qmix"
use_rnn: True
mixing_embed_dim: 32
hypernet_layers: 2
hypernet_embed: 64

# config for state encoder
encoder_use_rnn: True
encoder_hidden_dim: 32
ae_enc_hidden_dims: []
ae_dec_hidden_dims: []

attn_embed_dim: 16

concat_obs: True

state_repre_dim: 128
action_embed_dim: 8

state_encoder: ob_attn_ae

ob_embed_dim: 32

momentum_tau: 1

# configs about whether to use latent model learning
use_latent_model: True
use_rew_pred: True
use_momentum_encoder: True
use_residual: True
pred_len: 2
latent_model: mlp
model_hidden_dim: 64
spr_dim: 32


# configs about whether to use decorelation learning

decorelation_hidden_dim: 64
use_metamodel: False
use_decorelation : True

rl_signal: True

# config for trade-off among different losses
spr_coef: 1
rew_pred_coef: 1
repr_coef: 1
decorelation_coef: 0.005

# config about encoder testing/visualization
test_encoder: False

name: "DRMAC"

# config for robust communication
noise_env: False
noise_type: 0 # 
